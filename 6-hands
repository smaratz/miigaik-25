import requests
from bs4 import BeautifulSoup
import collections
import time
import re
from urllib.parse import urljoin, urlparse, urldefrag
import sys
# Глобальные переменные для управления скоростью HTTP-запросов.
last_request_time = 0.0
min_request_interval = 0.1  # Минимальный интервал между запросами (по умолчанию)

def set_rate_limit(requests_per_second):
    """Устанавливает частоту запросов в запросах в секунду."""
    global min_request_interval
    if requests_per_second > 0:
        min_request_interval = 1.0 / requests_per_second
    else:
        # Нет ограничения или очень маленькая задержка по умолчанию, если 0 rps нелогичен
        min_request_interval = 0.01


def fetch_page(url):
    """Загружает страницу, соблюдая ограничение скорости."""
    global last_request_time

    current_time = time.time()
    time_since_last_request = current_time - last_request_time
    if time_since_last_request < min_request_interval:
        sleep_duration = min_request_interval - time_since_last_request
        time.sleep(sleep_duration)

    try:
        # Устанавливаем заголовок User-Agent для Wikipedia
        headers = {
            'User-Agent': 'SearchWikipedia/1.0 (https://msaitgareev.ru/; smaratz11@gmail.com)'
        }
        response = requests.get(url, headers=headers, timeout=15)  # Таймаут 15 секунд
        response.raise_for_status()  # Вызовет исключение для HTTP ошибок (4xx, 5xx)
        last_request_time = time.time()
        return response.text
    except requests.RequestException as e:
        print(f"Ошибка при загрузке {url}: {e}")
        last_request_time = time.time()  # Обновляем время, чтобы избежать частого повтора при ошибках
        return None


def extract_wiki_links(html_content, base_url):
    """Извлекает валидные ссылки на статьи Wikipedia из HTML-контента."""
    links = set()
    if not html_content:
        return links

    soup = BeautifulSoup(html_content, 'html.parser')

    # Основной контент статей Wikipedia обычно находится в этом блоке
    content_div = soup.select_one('#mw-content-text .mw-parser-output')
    if not content_div:
        print(f"DEBUG: Не удалось найти '#mw-content-text .mw-parser-output' на странице.")
        content_div = soup.select_one('#bodyContent')  # Попытка с более общим селектором
        if not content_div:
            print(f"DEBUG: Не удалось найти основной блок контента ('#bodyContent').")
            return links

    for a_tag in content_div.find_all('a', href=True):
        href = a_tag['href']

        # Создаем абсолютный URL из относительного (например, /wiki/Article)
        full_url = urljoin(base_url, href)

        # Удаляем фрагмент URL (часть после #)
        full_url_no_fragment, _ = urldefrag(full_url)

        # Фильтр: только внутренние ссылки на статьи Википедии (начинаются с /wiki/, не спецстраницы)
        if not full_url_no_fragment.startswith(base_url + "/wiki/"):
            continue

        # Фильтр: только ссылки на статьи (эвристика: нет двоеточия в части после /wiki/)
        article_part = full_url_no_fragment.split('/wiki/', 1)[1]

        # Исключаем спецстраницы (File:, Category:, Template: и т.д.), содержащие ':'
        if ':' in article_part:
            continue

        links.add(full_url_no_fragment)

    return links


def find_path_bfs(start_url, end_url, base_url, max_hops=5):
    """Ищет путь между двумя URL с использованием BFS, не более 5 переходов."""

    start_url_norm, _ = urldefrag(urljoin(base_url, start_url))
    end_url_norm, _ = urldefrag(urljoin(base_url, end_url))

    if start_url_norm == end_url_norm:
        return [start_url_norm]  # 0 переходов

    queue = collections.deque([(start_url_norm, [start_url_norm])])
    visited = {start_url_norm}

    print(f"\nПоиск пути от {start_url_norm} до {end_url_norm} (макс. {max_hops} переходов)...")

    while queue:
        current_url, path_to_current_url = queue.popleft()

        hops_to_current_url = len(path_to_current_url) - 1
        html_content = fetch_page(current_url)
        if not html_content:
            print(f"  DEBUG: Не удалось загрузить контент для {current_url}")
            continue

        links_on_page = extract_wiki_links(html_content, base_url)
        # print(f"    DEBUG: Найдено {len(links_on_page)} ссылок на {current_url}")

        for next_link_candidate in sorted(list(links_on_page)):

            hops_for_new_path = hops_to_current_url + 1

            if next_link_candidate == end_url_norm:
                if hops_for_new_path <= max_hops:
                    return path_to_current_url + [next_link_candidate]
                else:
                    # Целевая статья найдена, но путь превышает допустимое число переходов.
                    continue

            if hops_for_new_path <= max_hops:
                if next_link_candidate not in visited:
                    visited.add(next_link_candidate)
                    queue.append((next_link_candidate, path_to_current_url + [next_link_candidate]))
                    # print(f"DEBUG: Добавлено в очередь: {next_link_candidate} ({hops_for_new_path} переходов)")
    return None
def main_script(url1_input, url2_input, rate_limit_rps_input):
    """Главная функция скрипта."""
    set_rate_limit(rate_limit_rps_input)

    parsed_initial_url = urlparse(url1_input)
    if not parsed_initial_url.scheme or not parsed_initial_url.netloc:
        print(f"Ошибка: Некорректный формат URL: {url1_input}")
        return
    base_wiki_url = f"{parsed_initial_url.scheme}://{parsed_initial_url.netloc}"

    max_allowed_hops = 5

    print(f"Начало поиска {url1_input} -> {url2_input}")
    path_1_to_2 = find_path_bfs(url1_input, url2_input, base_wiki_url, max_hops=max_allowed_hops)
    if path_1_to_2:
        print(f"Путь найден ({len(path_1_to_2) - 1} переходов): {' => '.join(path_1_to_2)}")
    else:
        print(f"Путь от {url1_input} до {url2_input} не найден за {max_allowed_hops} переходов.")

    print("-" * 40)

    print(f"Начало поиска {url2_input} -> {url1_input}")
    path_2_to_1 = find_path_bfs(url2_input, url1_input, base_wiki_url, max_hops=max_allowed_hops)
    if path_2_to_1:
        print(f"Путь найден ({len(path_2_to_1) - 1} переходов): {' => '.join(path_2_to_1)}")
    else:
        print(f"Путь от {url2_input} до {url1_input} не найден за {max_allowed_hops} переходов.")


if __name__ == '__main__':
    if len(sys.argv) != 4:
        print("Использование: python имя_скрипта.py <url1> <url2> <rate_limit_rps>")
        print(
            "Пример: python searchwikipedia.py https://en.wikipedia.org/wiki/Six_degrees_of_separation https://en.wikipedia.org/wiki/American_Broadcasting_Company 10")
        sys.exit(1)

    start_url_arg = sys.argv[1]
    end_url_arg = sys.argv[2]
    try:
        rate_limit_arg = int(sys.argv[3])
        if rate_limit_arg < 0:
            print("Ошибка: Ограничение скорости (rate_limit_rps) не может быть отрицательным.")
            sys.exit(1)
    except ValueError:
        print("Ошибка: Ограничение скорости (rate_limit_rps) должно быть целым числом.")
        sys.exit(1)

    main_script(start_url_arg, end_url_arg, rate_limit_arg)
